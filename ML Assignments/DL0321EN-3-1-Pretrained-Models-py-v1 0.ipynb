{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b927ad859f1349ff833559e06b6acb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5ed540be9b47f783d5ea6d15a4d826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize,image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 11:35:07.195721: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2024-08-02 11:35:07.200992: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394310000 Hz\n",
      "2024-08-02 11:35:07.205916: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55706c3d6690 executing computations on platform Host. Devices:\n",
      "2024-08-02 11:35:07.205954: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2024-08-02 11:35:07.233869: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7fb8ca9d4b50>,\n",
       " <keras.layers.core.Dense at 0x7fb8c86d7510>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fb937debb50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7fb9320adf10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb93009d450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb931859550>,\n",
       " <keras.layers.core.Activation at 0x7fb931859950>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fb93009d350>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb9316a1f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb9315c7150>,\n",
       " <keras.layers.core.Activation at 0x7fb9315c7110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb9315dc750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb93155be50>,\n",
       " <keras.layers.core.Activation at 0x7fb9315742d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb93005fc50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb928725a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb92876ded0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb9286e3090>,\n",
       " <keras.layers.merge.Add at 0x7fb9286e3bd0>,\n",
       " <keras.layers.core.Activation at 0x7fb92862df90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb92854fc50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb92853dfd0>,\n",
       " <keras.layers.core.Activation at 0x7fb9285a8b10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb9284c40d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb9284068d0>,\n",
       " <keras.layers.core.Activation at 0x7fb92843f590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb9283d8490>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb928375710>,\n",
       " <keras.layers.merge.Add at 0x7fb92833a910>,\n",
       " <keras.layers.core.Activation at 0x7fb9282d9d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb9281f9c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb9282503d0>,\n",
       " <keras.layers.core.Activation at 0x7fb928250490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb9281eedd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb928157a10>,\n",
       " <keras.layers.core.Activation at 0x7fb928157e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb928085810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb928067d50>,\n",
       " <keras.layers.merge.Add at 0x7fb928067a10>,\n",
       " <keras.layers.core.Activation at 0x7fb908746cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb9087465d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb9086d3e90>,\n",
       " <keras.layers.core.Activation at 0x7fb9086bea50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb908658a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb9085c0d90>,\n",
       " <keras.layers.core.Activation at 0x7fb9085c0f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb9317ca790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb908448f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb908527f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb9083c0210>,\n",
       " <keras.layers.merge.Add at 0x7fb908391a10>,\n",
       " <keras.layers.core.Activation at 0x7fb908345210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb9082f0650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb9082c3c50>,\n",
       " <keras.layers.core.Activation at 0x7fb9082dddd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb908205b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb908182dd0>,\n",
       " <keras.layers.core.Activation at 0x7fb9081da490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb908174810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb9080dcf90>,\n",
       " <keras.layers.merge.Add at 0x7fb9080dce10>,\n",
       " <keras.layers.core.Activation at 0x7fb8f07ce610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8f07ea990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8f07ae710>,\n",
       " <keras.layers.core.Activation at 0x7fb8f0748d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8f06f1f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8f0644650>,\n",
       " <keras.layers.core.Activation at 0x7fb8f0644510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8f05df650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8f0549f90>,\n",
       " <keras.layers.merge.Add at 0x7fb8f055cc50>,\n",
       " <keras.layers.core.Activation at 0x7fb8f0479d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8f0406250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8f045ef50>,\n",
       " <keras.layers.core.Activation at 0x7fb8f045e750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8f039a9d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8f036f310>,\n",
       " <keras.layers.core.Activation at 0x7fb8f036f450>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8f028af50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8f0206b90>,\n",
       " <keras.layers.merge.Add at 0x7fb8f0221410>,\n",
       " <keras.layers.core.Activation at 0x7fb8f01a5590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8f01a5690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8f011ec90>,\n",
       " <keras.layers.core.Activation at 0x7fb8f00e4f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8f0046e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8d07d84d0>,\n",
       " <keras.layers.core.Activation at 0x7fb8d07d8790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8d0778ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8d0611350>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8d06e0d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8d062ded0>,\n",
       " <keras.layers.merge.Add at 0x7fb8d062d610>,\n",
       " <keras.layers.core.Activation at 0x7fb8d0516f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8d04b49d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8d048db50>,\n",
       " <keras.layers.core.Activation at 0x7fb8d048d2d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8d03c5e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8d034d290>,\n",
       " <keras.layers.core.Activation at 0x7fb8d03a7650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8d02bfb10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8d0208b50>,\n",
       " <keras.layers.merge.Add at 0x7fb8d02b88d0>,\n",
       " <keras.layers.core.Activation at 0x7fb8d01d7250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8d0172710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8d014f590>,\n",
       " <keras.layers.core.Activation at 0x7fb8d014fe50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8d00ec650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8d0056f90>,\n",
       " <keras.layers.core.Activation at 0x7fb8d0068b50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cbf4dd50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cbedb250>,\n",
       " <keras.layers.merge.Add at 0x7fb8cbf34890>,\n",
       " <keras.layers.core.Activation at 0x7fb8cbe4ab90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cbe6df90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cbdc34d0>,\n",
       " <keras.layers.core.Activation at 0x7fb8cbdc31d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cbd07dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cbcda610>,\n",
       " <keras.layers.core.Activation at 0x7fb8cbcdafd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cbc74850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cbbdfed0>,\n",
       " <keras.layers.merge.Add at 0x7fb8cbbdfd50>,\n",
       " <keras.layers.core.Activation at 0x7fb8cbb10d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cbd07310>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cbaf7fd0>,\n",
       " <keras.layers.core.Activation at 0x7fb8cbaf79d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cba276d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb989f50>,\n",
       " <keras.layers.core.Activation at 0x7fb8cb926950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cb93bd90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb89f690>,\n",
       " <keras.layers.merge.Add at 0x7fb8cb89f5d0>,\n",
       " <keras.layers.core.Activation at 0x7fb8cb837890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cb7ee090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb7b66d0>,\n",
       " <keras.layers.core.Activation at 0x7fb8cb7b6ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cb6cfed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb648190>,\n",
       " <keras.layers.core.Activation at 0x7fb8cb648250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cb5e4510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb503a50>,\n",
       " <keras.layers.merge.Add at 0x7fb8cb53e950>,\n",
       " <keras.layers.core.Activation at 0x7fb8cb480ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cb49c690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb461f90>,\n",
       " <keras.layers.core.Activation at 0x7fb8cb4614d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cb398750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb378950>,\n",
       " <keras.layers.core.Activation at 0x7fb8cb378ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cb293810>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cb14ed10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb20c150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb164990>,\n",
       " <keras.layers.merge.Add at 0x7fb8cb0ac290>,\n",
       " <keras.layers.core.Activation at 0x7fb8cb041ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8caf89210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cb028cd0>,\n",
       " <keras.layers.core.Activation at 0x7fb8cb03ce50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8caf5c250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8caed3f50>,\n",
       " <keras.layers.core.Activation at 0x7fb8caeef250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cae72dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cad8f3d0>,\n",
       " <keras.layers.merge.Add at 0x7fb8cadd6150>,\n",
       " <keras.layers.core.Activation at 0x7fb8cad6dd90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cad6d950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cac85090>,\n",
       " <keras.layers.core.Activation at 0x7fb8cac47fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cac08650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8cab82390>,\n",
       " <keras.layers.core.Activation at 0x7fb8cab82790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb8cab1f510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb8caa98f90>,\n",
       " <keras.layers.merge.Add at 0x7fb8caabbbd0>,\n",
       " <keras.layers.core.Activation at 0x7fb8caa37d50>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7fb8cad6de90>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7fb932111450>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      " 19/101 [====>.........................] - ETA: 1:23:50 - loss: 0.2603 - acc: 0.9089"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
